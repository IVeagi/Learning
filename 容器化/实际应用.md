## 一  drain  节点排水 和 node affinity节点亲和性
  recommend应用普通deployment部署时候，延时较高 想通过在一个固定节点上测试，同时这个节点需要排除其他的pod来保证性能可靠
   1. 设置node1  drain 排除所有的现有pod （某些DS之类的忽略，也无法排除）
   2. 设置node1 标签如 test_app= recommend 
   3. 此应用的deployment 设置node affinity 选定标签为test_app =recommend  使用 requiredDurningSchedulingIngoredDurningExecution 强制选择此节点
      3.1 也可使用nodeSelector，过时了呀
   4.apply此deployment
   
## 二  从pod内获取pod 信息（包括pod ，node ，ns等） 
   应用日志使用的daemonset部署。通过hostpath挂载到/home/log，通过filebeat收集日志入ES，无法获取到产生此日志的node pod 信息
   1. 通多downwardAPI 获取环境变量  比如 env.valuefrom.fieldref:POD_NAME:metadata.name
   2. 部署改造后的deployment，进入容器，查看环境变量
   3. 改造应用代码，将pod node信息 注入到日志中

## 三 configmap 使用配置文件
     通过filebeat 收集日志的时候。将filebeat的配置文件挂载到configmap中  实现动态更改
     但是filebeat不会自动加载此文件  需要重新apply此daemonset
     
## 四 POD 的健康检查 livenessProbe 和 readinessProbe
     1.exec action
     2 tcpsocket action
     3 httpget action
     
## 五 IngressClass 的使用，选择ingeress控制器，annotation的使用
     使用过程中发现http的访问较ecs部署有很大的延时，查看资源使用，均处于较低水平。
     1. 阿里云部署的ingress ，使用的是SLB，但是默认的情况下机房是随机的（已咨询过阿里云，确实这么设计的，说是延时很低可忽略*****）
     2. 在相同区域先手动建立SLB
     3.然后创建ingress相关的资源。包含ingress需要在annotation中注释ingressclass 为新建h区域的实例
     4.再测试发现延时明显降低，较IDC环境相差不大

## 六 dns 
     使用域名访问的时候延时大，但是使用ip就不大
     开始集群内部的SVC沟通也是使用privatezone的dns解析，后来改成coredns 解析，但是业务高峰时期解析较慢，发现是coredns节点较少，
     增加deployment  pod数目，但是还是有节点没有分配到pod 此时使用node的本地dns缓存

## 七 pod驱逐机制， 并不考虑QOS等级，只根据nodefs使用量排序，驱逐最大的
    某个节点的pod 重启调度到了其他节点，
    1 在磁盘资源不足时驱逐  资源包括nodefs 和imagefs 每种资源包含available 和 inodesFree  
      默认情况下 nodefs.avaiable < 10%, nodefs.inodeFree < 5% imagefs.availables < 15% imgage.inodeFree < 15%
    2 内存不足的时候。 memory.available < 100Mi
